{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from nilearn import plotting\n",
    "from nilearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurolang import frontend as fe\n",
    "\n",
    "from nilearn.datasets import utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare NeuroSynth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_neurosynth = utils._get_dataset_dir('neurosynth', data_dir='neurolang_data')\n",
    "\n",
    "f_neurosynth = utils._fetch_files(\n",
    "    d_neurosynth, [\n",
    "        (\n",
    "            f,\n",
    "            'https://github.com/neurosynth/neurosynth-data/raw/master/current_data.tar.gz',\n",
    "            {'uncompress': True}\n",
    "        )\n",
    "        for f in ('database.txt', 'features.txt')\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "database = pd.read_csv(f_neurosynth[0], sep='\\t')\n",
    "features = pd.read_csv(f_neurosynth[1], sep='\\t')\n",
    "\n",
    "features_normalised = (\n",
    "    features\n",
    "    .melt(id_vars=features.columns[0], var_name='term', value_vars=features.columns[1:], value_name='tfidf')\n",
    "    .query('tfidf > 0')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dwasserm/anaconda/envs/root3.8-neurosynth/lib/python3.8/site-packages/neurosynth/base/dataset.py:769: FutureWarning: DataFrame.to_sparse is deprecated and will be removed in a future version\n",
      "  self.data = pd.DataFrame(self.data['values'].todense(),\n"
     ]
    }
   ],
   "source": [
    "nsh = fe.neurosynth_utils.NeuroSynthHandler()\n",
    "ns_ds = nsh.ns_load_dataset()\n",
    "it = ns_ds.image_table\n",
    "vox_ids, study_ids_ix = it.data.nonzero()\n",
    "study_ids = ns_ds.image_table.ids[study_ids_ix]\n",
    "study_id_vox_id = np.transpose([study_ids, vox_ids])\n",
    "masked_ = it.masker.unmask(np.arange(it.data.shape[0]))\n",
    "nnz = masked_.nonzero()\n",
    "vox_id_MNI = np.c_[\n",
    "    masked_[nnz].astype(int),\n",
    "    nib.affines.apply_affine(it.masker.volume.affine, np.transpose(nnz)),\n",
    "    [\n",
    "        fe.ExplicitVBR(\n",
    "            [v],\n",
    "            affine_matrix=it.masker.volume.affine,\n",
    "            image_dim=it.masker.volume.shape\n",
    "        )\n",
    "        for v in zip(*nnz)\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise and load the fronte-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = fe.NeurolangDL()\n",
    "\n",
    "@nl.add_symbol\n",
    "def agg_count(x: Iterable) -> int:\n",
    "    return len(x)\n",
    "\n",
    "@nl.add_symbol\n",
    "def agg_sum(x: Iterable) -> float:\n",
    "    return x.sum()\n",
    "\n",
    "@nl.add_symbol\n",
    "def agg_mean(x: Iterable) -> float:\n",
    "    return x.mean()\n",
    "\n",
    "ns_pmid_term_tfidf = nl.add_tuple_set(features_normalised.values, name='ns_pmid_term_tfidf')\n",
    "ns_activations = nl.add_tuple_set(database[['id', 'x', 'y', 'z', 'space']].values, name='ns_activations')\n",
    "ns_activations_by_id = nl.add_tuple_set(\n",
    "    study_id_vox_id, name='ns_activations_by_id'\n",
    ")\n",
    "ns_vox_id_MNI = nl.add_tuple_set(vox_id_MNI, name='ns_vox_id_MNI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward inference on term \"Auditory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nl.scope as e:\n",
    "    e.term_docs[e.term, e.pmid] = (\n",
    "        ns_pmid_term_tfidf[e.pmid, e.term, e.tfidf] &\n",
    "        (e.term == 'auditory') &\n",
    "        (e.tfidf > 1e-3)        \n",
    "    )\n",
    "\n",
    "    e.act_term_counts[e.term, e.voxid, agg_count(e.pmid)] = (\n",
    "        ns_activations_by_id[e.pmid, e.voxid] &\n",
    "        e.term_docs[e.term, e.pmid]\n",
    "    )    \n",
    "\n",
    "    e.term_counts[e.term, agg_count(e.pmid)] =  (\n",
    "        ns_pmid_term_tfidf[e.pmid, e.term, e.tfidf] &\n",
    "        e.term_docs[e.term, e.pmid]\n",
    "    )\n",
    " \n",
    "    e.p_act_given_term[e.voxid, e.x, e.y, e.z, e.term, e.prob] = (\n",
    "        e.act_term_counts[e.term, e.voxid, e.act_term_count] &\n",
    "        e.term_counts[e.term, e.term_count] &\n",
    "        e.ns_vox_id_MNI[e.voxid, e.x, e.y, e.z] &\n",
    "        (e.prob == (e.act_term_count / e.term_count))\n",
    "    )\n",
    "    res = nl.solve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mni_t1 = it.masker.volume\n",
    "\n",
    "ijk_coords = tuple(\n",
    "    (\n",
    "        np.round(\n",
    "            nib.affines.apply_affine(\n",
    "                np.linalg.inv(mni_t1.affine),\n",
    "                res['p_act_given_term'].unwrap().as_numpy_array()[:, 1:4]\n",
    "            ).astype(float).T\n",
    "        ).T\n",
    "    ).astype(int).T\n",
    ")\n",
    "\n",
    "res_img = np.zeros(mni_t1.shape)\n",
    "res_img[ijk_coords] = res['p_act_given_term'].unwrap().as_numpy_array()[:, -1]\n",
    "\n",
    "res_sp = nib.spatialimages.SpatialImage(\n",
    "    dataobj=res_img, affine=mni_t1.affine\n",
    ")\n",
    "\n",
    "plotting.plot_stat_map(res_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Inference Terms Associated with Voxels Associated with \"Auditory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res['p_act_given_term'].unwrap()\n",
    "thresh_voxels = r.selection(lambda x: x[5] > .12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auditory_voxels_prob = nl.add_tuple_set(thresh_voxels.as_numpy_array(), name='auditory_voxels_prob');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%snakeviz\n",
    "with nl.scope as e:\n",
    "    e.term_docs[e.term, e.pmid] = (\n",
    "        ns_pmid_term_tfidf[e.pmid, e.term, e.tfidf] &\n",
    "        (e.tfidf > 1e-3)        \n",
    "    )\n",
    "\n",
    "    e.term_counts[e.term, agg_count(e.pmid)] =  (\n",
    "        ns_pmid_term_tfidf[e.pmid, e.term, e.tfidf] &\n",
    "        e.term_docs[e.term, e.pmid]\n",
    "    )\n",
    "    e.act_counts[e.voxid, agg_count(e.pmid)] = ns_activations_by_id[e.pmid, e.voxid]\n",
    "    \n",
    "    e.quantity_docs[agg_count(e.pmid)] = ns_pmid_term_tfidf[e.pmid, e.term, e.tfidf]\n",
    "    \n",
    "    e.act_prob[e.voxid, e.prob] = (\n",
    "        e.act_counts[e.voxid, e.count] &\n",
    "        e.quantity_docs[e.q] &\n",
    "        (e.prob == e.count / e.q)\n",
    "    )\n",
    "    \n",
    "    e.act_term_counts[e.voxid, e.term, agg_count(e.pmid)] = (\n",
    "        auditory_voxels_prob[e.voxid, e.x, e.y, e.z, e.term_, e.prob_] &\n",
    "        e.term_docs[e.term, e.pmid] &\n",
    "        ns_activations_by_id[e.pmid, e.voxid]\n",
    "    )\n",
    "\n",
    "    e.p_term_given_act[e.term, e.voxid, e.prob] = (\n",
    "        e.act_term_counts[e.voxid, e.term, e.act_term_count] &\n",
    "        e.act_counts[e.voxid, e.act_count] &\n",
    "        (e.prob == e.act_term_count / e.act_count)\n",
    "    )\n",
    "    \n",
    "    e.e_term_given_aud_act[e.term, agg_sum(e.p)] = (\n",
    "        e.p_term_given_act[e.term, e.voxid, e.prob] & \n",
    "        e.act_prob[e.voxid, e.act_prob_] &\n",
    "        (e.p == e.prob * e.act_prob_)    \n",
    "    )\n",
    "    \n",
    "    res = nl.solve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['e_term_given_aud_act']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = res['e_term_given_aud_act']._container.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the terms that are on the top 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[1].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[c[1] >= c[1].quantile(.99)].sort_values(1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
