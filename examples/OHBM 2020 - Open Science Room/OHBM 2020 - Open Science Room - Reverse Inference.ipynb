{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/nilearn/plotting/cm.py:159: MatplotlibDeprecationWarning: \n",
      "The revcmap function was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use Colormap.reversed() instead.\n",
      "  _cmaps_data[_cmapname_r] = _cm.revcmap(_cmapspec)\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/tatsu/grammars.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import defaultdict, Mapping\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/problog/util.py:250: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class OrderedSet(collections.MutableSet):\n"
     ]
    }
   ],
   "source": [
    "import stats_helper, datasets_helper\n",
    "from neurolang.frontend.probabilistic_frontend import ProbabilisticFrontend\n",
    "from rdflib import RDFS\n",
    "from nilearn import plotting\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Iterable\n",
    "from neurolang import frontend as fe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the FMA ontology to obtain regions of the brain included within the `Temporal lobe`. We will obtain all the entities that make up the `Temporal Lobe` and then we will convert them into regions using the information provided by the Destrieux atlas. This will allow us to perform spatial operations on these regions, allowing us to obtain those NeuroSynth regions associated with the term `auditory` that overlap our results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/neurosynth/base/mask.py:232: DeprecationWarning: get_header method is deprecated.\n",
      "Please use the ``img.header`` property instead.\n",
      "\n",
      "* deprecated from version: 2.1\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 4.0\n",
      "  return self.volume.get_header()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-666a596bc1d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProbabilisticFrontend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdatasets_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_reverse_inference_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/INRIA/NeuroLang/examples/OHBM 2020 - Open Science Room/datasets_helper.py\u001b[0m in \u001b[0;36mload_reverse_inference_dataset\u001b[0;34m(nl)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mnsh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurosynth_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeuroSynthHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0msample_studies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnsh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_study_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0msample_studies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_studies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0msample_studies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_studies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'study'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/INRIA/NeuroLang/neurolang/frontend/neurosynth_utils.py\u001b[0m in \u001b[0;36mns_study_ids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mns_study_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         return np.expand_dims(\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStudyID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/INRIA/NeuroLang/neurolang/frontend/neurosynth_utils.py\u001b[0m in \u001b[0;36mdataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_load_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/INRIA/NeuroLang/neurolang/frontend/neurosynth_utils.py\u001b[0m in \u001b[0;36mns_load_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m\"Downloading neurosynth database\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         )\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_ns_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/INRIA/NeuroLang/neurolang/frontend/neurosynth_utils.py\u001b[0m in \u001b[0;36mdownload_ns_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m         )\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_neurosynth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_neurosynth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neurolang/lib/python3.7/site-packages/neurosynth/base/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, feature_filename, masker, r, transform, target, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Create supporting tables for images and features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_image_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeature_filename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/neurolang/lib/python3.7/site-packages/neurosynth/base/dataset.py\u001b[0m in \u001b[0;36mcreate_image_table\u001b[0;34m(self, r)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     def get_studies(self, features=None, expression=None, mask=None,\n",
      "\u001b[0;32m~/miniconda3/envs/neurolang/lib/python3.7/site-packages/neurosynth/base/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, r, use_sparse)\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0mvals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_masked\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0mrows\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                 \u001b[0mcols\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_masked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nl = ProbabilisticFrontend()\n",
    "datasets_helper.load_reverse_inference_dataset(nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'neurolang_data/ontologies/cogat.xrdf'\n",
    "nl.load_ontology(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nl.scope as e:\n",
    "    e.julich_to_neurosynth[e.julich_id, e.id_neurosynth, e.x, e.y, e.z] = (\n",
    "        e.xyz_julich[e.x, e.y, e.z, e.julich_id] &\n",
    "        e.xyz_neurosynth[e.x, e.y, e.z, e.id_neurosynth]\n",
    "    )\n",
    "    \n",
    "    e.region_voxels[e.name, e.id_neurosynth, e.x, e.y, e.z] = (\n",
    "        e.julich_id[e.name, e.julich_id] &\n",
    "        e.julich_to_neurosynth[e.julich_id, e.id_neurosynth, e.x, e.y, e.z]\n",
    "    )\n",
    "    \n",
    "    e.julich_id[e.name, e.id] = (\n",
    "        e.julich_ontology[e.name, 'labelIndex', e.id]\n",
    "    )\n",
    "    \n",
    "    e.julich_voxels[e.id_neurosynth, e.x, e.y, e.z] = (\n",
    "        e.region_voxels['Area Ia (Insula)', e.id_neurosynth, e.x, e.y, e.z]\n",
    "    )\n",
    "    \n",
    "    e.p_act[e.id_voxel, e.term] = (\n",
    "        e.p_voxel_study[e.id_voxel, e.id_study] & \n",
    "        e.p_term_study[e.term,  e.id_study] & \n",
    "        e.p_study[e.id_study]\n",
    "    )\n",
    "    \n",
    "    e.probability_voxel[e.term] = (\n",
    "        e.p_act[e.id_voxel, e.term] &\n",
    "        e.julich_voxels[e.id_voxel, e.x, e.y, e.z]\n",
    "    )\n",
    "    \n",
    "    nl_results = nl.solve_query(e.probability_voxel[e.term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>fresh_00004571</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>8.259320e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>8.654360e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05</td>\n",
       "      <td>1.696883e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>3.720645e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>1.569163e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  term  fresh_00004571\n",
       "0  001    8.259320e-08\n",
       "1   01    8.654360e-08\n",
       "2   05    1.696883e-07\n",
       "3   10    3.720645e-07\n",
       "4  100    1.569163e-07"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_results.value.as_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/neurosynth/base/dataset.py:624: FutureWarning: DataFrame/Series.to_dense is deprecated and will be removed in a future version\n",
      "  old_data = self.data.to_dense()\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/neurosynth/base/dataset.py:634: FutureWarning: DataFrame.to_sparse is deprecated and will be removed in a future version\n",
      "  self.data = data.fillna(0.0).to_sparse()\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/neurosynth/base/mask.py:232: DeprecationWarning: get_header method is deprecated.\n",
      "Please use the ``img.header`` property instead.\n",
      "\n",
      "* deprecated from version: 2.1\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 4.0\n",
      "  return self.volume.get_header()\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/neurosynth/base/dataset.py:624: FutureWarning: DataFrame/Series.to_dense is deprecated and will be removed in a future version\n",
      "  old_data = self.data.to_dense()\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/neurosynth/base/dataset.py:634: FutureWarning: DataFrame.to_sparse is deprecated and will be removed in a future version\n",
      "  self.data = data.fillna(0.0).to_sparse()\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/numpy/lib/npyio.py:2358: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  output = genfromtxt(fname, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import stats_helper, datasets_helper\n",
    "from neurolang.frontend.probabilistic_frontend import ProbabilisticFrontend\n",
    "from rdflib import RDFS\n",
    "from nilearn import plotting\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Iterable\n",
    "from neurolang import frontend as fe\n",
    "\n",
    "nl = ProbabilisticFrontend()\n",
    "datasets_helper.load_reverse_inference_dataset(nl)\n",
    "\n",
    "path = 'neurolang_data/ontologies/cogat.xrdf'\n",
    "nl.load_ontology(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_of = nl.new_symbol(name='http://www.obofoundry.org/ro/ro.owl#part_of')\n",
    "subclass_of = nl.new_symbol(name=str(RDFS.subClassOf))\n",
    "label = nl.new_symbol(name=str(RDFS.label))\n",
    "hasTopConcept = nl.new_symbol(name='http://www.w3.org/2004/02/skos/core#hasTopConcept')\n",
    "\n",
    "@nl.add_symbol\n",
    "def word_lower(name1: str, name2: str) -> bool:\n",
    "    print('-->', name1, name2)\n",
    "    if str(name1).lower() == str(name2).lower():\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gzanitti/Projects/INRIA/NeuroLang/neurolang/datalog/aggregation.py:152: UserWarning: No check performed. Should implement check for stratified aggregation\n",
      "  \"No check performed. Should implement check for stratified\"\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/pandas/core/frame.py:4218: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from operator import eq\n",
    "\n",
    "with nl.scope as e:\n",
    "    e.julich_to_neurosynth[e.julich_id, e.id_neurosynth, e.x, e.y, e.z] = (\n",
    "        e.xyz_julich[e.x, e.y, e.z, e.julich_id] &\n",
    "        e.xyz_neurosynth[e.x, e.y, e.z, e.id_neurosynth]\n",
    "    )\n",
    "    \n",
    "    e.region_voxels[e.name, e.id_neurosynth, e.x, e.y, e.z] = (\n",
    "        e.julich_id[e.name, e.julich_id] &\n",
    "        e.julich_to_neurosynth[e.julich_id, e.id_neurosynth, e.x, e.y, e.z]\n",
    "    )\n",
    "    \n",
    "    e.julich_id[e.name, e.id] = (\n",
    "        e.julich_ontology[e.name, 'labelIndex', e.id]\n",
    "    )\n",
    "    \n",
    "    e.julich_voxels[e.id_neurosynth, e.x, e.y, e.z] = (\n",
    "        e.region_voxels['Area Ia (Insula)', e.id_neurosynth, e.x, e.y, e.z]\n",
    "    )\n",
    "    \n",
    "    e.p_act[e.id_voxel, e.term] = (\n",
    "        e.p_voxel_study[e.id_voxel, e.id_study] & \n",
    "        e.p_term_study[e.term,  e.id_study] & \n",
    "        e.p_study[e.id_study]\n",
    "    )\n",
    "    \n",
    "    e.ontology_terms[e.name] = (\n",
    "        hasTopConcept[e.uri, 'Executive-Cognitive Control'] &\n",
    "        label[e.uri, e.name]\n",
    "    )\n",
    "    \n",
    "    #e.probability_voxel[e.term1] = (\n",
    "    #    e.p_act[e.id_voxel, e.term1] &\n",
    "    #    e.julich_voxels[e.id_voxel, e.x, e.y, e.z] &\n",
    "    #    e.ontology_terms[e.term2] &\n",
    "    #    word_lower[e.term1, e.term2]\n",
    "    #)\n",
    "    \n",
    "    nl_results = nl.solve_query(e.p_act[e.id_voxel, e.term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = nl.new_symbol(name=str(RDFS.label))\n",
    "subclass_of = nl.new_symbol(name=str(RDFS.subClassOf))\n",
    "regional_part = nl.new_symbol(name='http://sig.biostr.washington.edu/fma3.0#regional_part_of')\n",
    "\n",
    "#@nl.add_symbol\n",
    "#def agg_create_region(x: Iterable, y: Iterable, z: Iterable) -> fe.ExplicitVBR:\n",
    "#    mni_t1 = it.masker.volume\n",
    "#    voxels = nib.affines.apply_affine(np.linalg.inv(mni_t1.affine), np.c_[x, y, z])\n",
    "#    return fe.ExplicitVBR(voxels, mni_t1.affine, image_dim=mni_t1.shape)\n",
    "\n",
    "@nl.add_symbol\n",
    "def first_word(name: str) -> str:\n",
    "    return name.split(\" \")[0]\n",
    "\n",
    "with nl.environment as e:    \n",
    "    e.fma_related_region[e.subregion_name, e.fma_uri] = (\n",
    "        label(e.xfma_entity_name, e.fma_uri) & \n",
    "        regional_part(e.fma_region, e.xfma_entity_name) & \n",
    "        subclass_of(e.fma_subregion, e.fma_region) &\n",
    "        label(e.fma_subregion, e.subregion_name)\n",
    "    )\n",
    "    e.fma_related_region[e.recursive_region, e.fma_name] = (\n",
    "        subclass_of(e.recursive_region, e.fma_subregion) & e.fma_related_region(e.fma_subregion, e.fma_name)\n",
    "    )\n",
    "    e.fma_to_destrieux[e.fma_name, e.destrieux_name] = (\n",
    "        label(e.fma_uri, e.fma_name) & e.relation_destrieux_fma(e.destrieux_name, e.fma_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gzanitti/Projects/INRIA/NeuroLang/neurolang/datalog/aggregation.py:146: UserWarning: No check performed. Should implement check for stratified aggregation\n",
      "  \"No check performed. Should implement check for stratified\"\n",
      "/Users/gzanitti/miniconda3/envs/neurolang/lib/python3.7/site-packages/pandas/core/frame.py:4218: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "with nl.environment as e:\n",
    "    e.region_voxels[e.id_neurosynth, e.x, e.y, e.z] = (\n",
    "        e.fma_related_region[e.fma_subregions, 'Temporal lobe'] & \n",
    "        e.fma_to_destrieux[e.fma_subregions, e.destrieux_name] & \n",
    "        e.destrieux_to_neurosynth[e.destrieux_name, e.id_neurosynth, e.x, e.y, e.z]\n",
    "    )\n",
    "    \n",
    "    e.destrieux_to_neurosynth[e.destrieux_name, e.id_neurosynth, e.x, e.y, e.z] = (\n",
    "        e.destrieux_labels[e.id_destrieux, e.destrieux_name] &\n",
    "        e.xyz_destrieux[e.x, e.y, e.z, e.id_destrieux] &\n",
    "        e.xyz_neurosynth[e.x, e.y, e.z, e.id_neurosynth]\n",
    "    )\n",
    "    \n",
    "    e.p_act[e.id_voxel, e.term, e.id_study] = (\n",
    "        e.p_voxel_study[e.id_voxel, e.id_study] & \n",
    "        e.p_term_study[e.term,  e.id_study] & \n",
    "        e.p_study[e.id_study]\n",
    "    )\n",
    "    \n",
    "    e.probability_voxel[e.id_voxel, e.x, e.y, e.z] = (\n",
    "        e.p_act[e.id_voxel, e.term, e.id_study] &\n",
    "        e.region_voxels[e.id_voxel, e.x, e.y, e.z]\n",
    "    )\n",
    "    \n",
    "    #nl_results = nl.solve_all()\n",
    "    nl_results = nl.solve_query(e.probability_voxel[e.id_voxel, e.x, e.y, e.z])\n",
    "    \n",
    "    #e.probability_voxel[nl.symbols.agg_create_region(e.x, e.y, e.z)] = (\n",
    "    #    e.p_act[e.id_voxel, e.term, e.id_study] &\n",
    "    #    e.region_voxels[e.id_voxel, e.x, e.y, e.z]\n",
    "    #)\n",
    "    \n",
    "    #e.final[e.region] = e.probability_voxel[e.region]\n",
    "    \n",
    "    #nl_results = nl.solve_query(e.final[e.region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = nl_results.value._container.values\n",
    "f = [(float(prob), id_voxel, x, y, z) for z, id_voxel, x, y, prob in t]\n",
    "p_act_aud = nl.add_probabilistic_facts_from_tuples(tuple(f), name='p_act_aud');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_img_nl = datasets_helper.parse_results(nl_results)\n",
    "plotting.plot_stat_map(\n",
    "    prob_img_nl, \n",
    "    title='Tag \"auditory\" (Neurolang)', \n",
    "    cmap='PuBuGn',\n",
    "    display_mode='x',\n",
    "    cut_coords=np.linspace(-63, 63, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    prob_img_nl, title='Tag \"auditory\" (Neurolang)', \n",
    "    cmap='PuBuGn',\n",
    "    display_mode='y',\n",
    "    cut_coords=np.linspace(-30, 5, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import RDF\n",
    "\n",
    "part_of = nl.new_symbol(name='http://www.obofoundry.org/ro/ro.owl#part_of')\n",
    "\n",
    "# Should found a better to imply this\n",
    "triples = nl.symbol_table[nl.get_ontology_triples_symbol().name]\n",
    "a = triples.value.as_numpy_array()\n",
    "t = [('Auditory', str(RDF.type), 'http://www.cognitiveatlas.org/ontology/cogat.owl#CAO_00148')]\n",
    "\n",
    "t = np.concatenate((a, t))\n",
    "nl.add_extensional_predicate_from_tuples(t, name=nl.get_ontology_triples_symbol().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nl.scope as e:\n",
    "    e.pre_part[e.x, e.y] = part_of[e.x, e.y]\n",
    "\n",
    "    e.perception_terms[e.short_name] = (\n",
    "        e.pre_part[\"Auditory\", e.y] & \n",
    "        subclass_of[e.z, e.y] & \n",
    "        label(e.z, e.term) &\n",
    "        (e.short_name == nl.symbols.first_word(e.term))\n",
    "    )\n",
    "    \n",
    "    e.p_term_given_act[e.term, e.voxid] = (\n",
    "        e.ns_reported_activations[e.study, e.voxid] &\n",
    "        e.perception_terms[e.term] & \n",
    "        e.ns_term_study_associations[e.study, e.term]\n",
    "    )\n",
    "    \n",
    "    e.p_term_g_aud_voxels[e.term] = (\n",
    "        e.p_term_given_act[e.term, e.voxid] &\n",
    "        e.p_act_aud[e.voxid, e.x, e.y, e.z]\n",
    "    )\n",
    "    \n",
    "    nl_reverse = nl.solve_query(e.p_term_g_aud_voxels[e.term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#prob_terms, prob_voxels, prob_terms_voxels = stats_helper.load_neurosynth_database()\n",
    "prob_img = stats_helper.parse_neurolang_result(result, prob_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    prob_img, \n",
    "    title='Tag \"auditory\" (Neurolang)', \n",
    "    cmap='PuBuGn',\n",
    "    display_mode='x',\n",
    "    cut_coords=np.linspace(-63, 63, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    prob_img, title='Tag \"auditory\" (Neurolang)', \n",
    "    cmap='PuBuGn',\n",
    "    display_mode='y',\n",
    "    cut_coords=np.linspace(-30, 5, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the same result obtained directly from the NeuroSynth database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_img_ns = stats_helper.parse_neurosynth_result(prob_terms_voxels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    prob_img_ns, title='Tag \"auditory\" (Neurosynth)', \n",
    "    cmap='PuBu',\n",
    "    display_mode='x',\n",
    "    cut_coords=np.linspace(-63, 63, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    prob_img_ns, title='Tag \"auditory\" (Neurosynth)', \n",
    "    cmap='PuBu',\n",
    "    display_mode='y',\n",
    "    cut_coords=np.linspace(-30, 5, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can analyze the results by plotting the p-values obtained. Let's start with the NeuroLang results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, p_values_corrected, p_value_image = stats_helper.compute_p_values(prob_img, q=1e-25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(-np.log10(res))\n",
    "plt.axvline(-np.log10(p_values_corrected), c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    p_value_image, \n",
    "    title=r'$-\\log_{10} P$ value (Neurolang)', \n",
    "    threshold=-np.log10(p_values_corrected), \n",
    "    cmap='YlOrRd',\n",
    "    display_mode='x',\n",
    "    cut_coords=np.linspace(-63, 63, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    p_value_image, title=r'$-\\log_{10} P$ value (Neurolang)', \n",
    "    threshold=-np.log10(p_values_corrected),\n",
    "    cmap='YlOrRd',\n",
    "    display_mode='y',\n",
    "    cut_coords=np.linspace(-30, 5, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above results, we can see that the regions have a high specificity and that they focus entirely on our area of interest. Reducing the area of work in this way allows us to minimize variance, enabling us to obtain results with greater statistical power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's do the same with the NeuroSynth results to compare. It is important to mention that the techniques used for the calculation of the p-values, make a comparison against the average of the activations. Bearing this in mind, by decreasing the region to be analyzed and focusing it on the activated region, the average of the activations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, p_values_corrected, p_value_image = stats_helper.compute_p_values(prob_img_ns, q=1e-25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(-np.log10(res))\n",
    "plt.axvline(-np.log10(p_values_corrected), c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    p_value_image, \n",
    "    title=r'$-\\log_{10} P$ value (NeuroSynth)', \n",
    "    threshold=-np.log10(p_values_corrected), \n",
    "    cmap='YlOrRd',\n",
    "    display_mode='x',\n",
    "    cut_coords=np.linspace(-63, 63, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(\n",
    "    p_value_image, title=r'$-\\log_{10} P$ value (NeuroSynth)', \n",
    "    threshold=-np.log10(p_values_corrected),\n",
    "    cmap='YlOrRd',\n",
    "    display_mode='y',\n",
    "    cut_coords=np.linspace(-30, 5, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen above how despite using a restrictive threshold for the p-value ($q<10^{25}$, FDR corrected), in the Neurosynth example there are activations considered statistically significant in the motor cortex that should not be present for the `auditory` tag. Using a prior information in NeuroLang, we are able to remove these false positives and obtain a cleaner result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "[1] Yarkoni, T.: Neurosynth core tools v0.3.1, DOI: 10.5281/zenodo.9925 (2014). <br/>\n",
    "[2] Yarkoni, T., Poldrack, R. A., Nichols, T. E., Van Essen, D. C. & Wager, T. D: Large-scale automated synthesis of human functional neuroimaging data. Nat. Methods 8, 665–670, DOI: 10.1038/nmeth.1635 (2011). <br/>\n",
    "[3] News. Journal of Investigative Medicine 58 (8), 929 (Dec2010). https://doi.org/10.2310/JIM.0b013e3182025955, http://jim.bmj.com/content/58/8/929.abstract <br/>\n",
    "[4] Insel, T. R., Landis, S.C., Collins, F.S.: Research priorities. The NIHBRAIN Initiative. Science (New York, N.Y.) 340 (6133), 687–688 (May  2013). https://doi.org/10.1126/science.1239276 <br/>\n",
    "[5] Markram, H.: The human brain project. Scientific American306(6), 50–55 (Jun2012). https://doi.org/10.1038/scientificamerican0612-50\n",
    "[6] Derrfuss, J. & Mar, R. A. Lost in localization: the need for a universal coordinate database. NeuroImage 48, 1–7, DOI:10.1016/j.neuroimage.2009.01.053 (2009)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "neurolang",
   "language": "python",
   "name": "neurolang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
